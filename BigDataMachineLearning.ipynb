{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CZĘŚĆ 1 - OPTYMALIZACJA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## OPIS ZESTAWU DANYCH\n",
    "Dane składają się z informacji o przylotach i odlotach wszystkich lotów komercyjnych w USA od października 1987 do kwietnia 2008 – przede wszystkim o ich opóźnieniach. \\\n",
    "Zbiór danych jest bardzo duży (120mln rekordów, 12GB danych) – na potrzeby projektu wykorzystamy jedynie dane z roku 2007 co ograniczy rozmiar przetwarzanych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn import preprocessing\n",
    "import pyarrow as pa\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib import pyplot as plt\n",
    "import statistics\n",
    "from sklearn.impute import KNNImputer\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnull, when, count, col, hour, mean, lit, stddev,abs\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, DecisionTreeRegressor\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.sql.types import IntegerType, StructField, LongType, DoubleType, StructType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### OPIS KLAS\n",
    "W sumie klas jest 29, opisują one następujące informacje:\n",
    "* rok\n",
    "* miesiąc\n",
    "* dzień miesiąca\n",
    "* dzień tygodnia\n",
    "* rzeczywisty czas odlotu\n",
    "* zaplanowany czas odlotu\n",
    "* rzeczywisty czas przylotu\n",
    "* zaplanowany czas przylotu\n",
    "* kod przewoźnika\n",
    "* numer lotu\n",
    "* numer ogonowy samolotu\n",
    "* całkowity czas lotu w minutach\n",
    "* rzeczywisty czas lotu\n",
    "* całkowity czas w powietrzu\n",
    "* opóźnienie lotu w minutach\n",
    "* miejsce startu\n",
    "* miejsce docelowe\n",
    "* odległość w milach\n",
    "* dane dotyczące przyjazdu taksówki\n",
    "* informacje o tym czy lot był anulowany\n",
    "* powód anulowania (pogoda, przewoźnik, ochrona, NAS)\n",
    "* przekierowanie (tak/nie)\n",
    "* opóźnienie przewoźnika w minutach\n",
    "* opóźnienie pogodowe w minutach\n",
    "* opóźnienie NAS w minutach\n",
    "* opóźnienie z powodów bezpieczeństwa w minutach\n",
    "* sumaryczne opóźnienie w minutach\n",
    "\n",
    "Celem projektu jest przewidywanie sumarycznego opóźnienia samolotu - zmienna objaśniana - na podstawie podzbioru pozostałych kolumn (zmiennych objaśniających), które wybierzemy na podstawie dalszej analizy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "columns =  [\"Year\", \"Month\", \"DayofMonth\", \"DayOfWeek\", \"DepTime\", \"CRSDepTime\", \"ArrTime\", \"CRSArrTime\", \"UniqueCarrier\", \"FlightNum\", \"TailNum\", \"ActualElapsedTime\", \"CRSElapsedTime\", \"AirTime\", \"ArrDelay\", \"DepDelay\", \"Origin\", \"Dest\", \"Distance\", \"TaxiIn\", \"TaxiOut\", \"Cancelled\", \"CancellationCode\", \"Diverted\", \"CarrierDelay\", \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Project\").config(\"spark.driver.memory\", \"15g\").getOrCreate()\n",
    "sparkDF = spark.read.orc(\"data2006-2008.orc\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "sparkDF = spark.createDataFrame(data = sparkDF.rdd, schema = columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "sparkDF = sparkDF.filter(sparkDF.Year!='Year')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: integer (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: integer (nullable = true)\n",
      " |-- WeatherDelay: integer (nullable = true)\n",
      " |-- NASDelay: integer (nullable = true)\n",
      " |-- SecurityDelay: integer (nullable = true)\n",
      " |-- LateAircraftDelay: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: integer (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: integer (nullable = true)\n",
      " |-- WeatherDelay: integer (nullable = true)\n",
      " |-- NASDelay: integer (nullable = true)\n",
      " |-- SecurityDelay: integer (nullable = true)\n",
      " |-- LateAircraftDelay: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in columns:\n",
    "    if c in [\"UniqueCarrier\", \"TailNum\", \"Origin\", \"Dest\"]:\n",
    "        sparkDF = sparkDF.withColumn(c,col(c).cast(StringType()))\n",
    "    else:\n",
    "        sparkDF = sparkDF.withColumn(c,col(c).cast(IntegerType()))\n",
    "sparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Month  DayofMonth  DayOfWeek  DepTime  CRSDepTime  ArrTime  \\\n",
      "0  2008      1           3          4     1343        1325     1451   \n",
      "1  2008      1           3          4     1125        1120     1247   \n",
      "2  2008      1           3          4     2009        2015     2136   \n",
      "3  2008      1           3          4      903         855     1203   \n",
      "4  2008      1           3          4     1423        1400     1726   \n",
      "\n",
      "   CRSArrTime UniqueCarrier  FlightNum  ... TaxiIn  TaxiOut  Cancelled  \\\n",
      "0        1435            WN        588  ...      4        9          0   \n",
      "1        1245            WN       1343  ...      3        8          0   \n",
      "2        2140            WN       3841  ...      2       14          0   \n",
      "3        1205            WN          3  ...      5        7          0   \n",
      "4        1710            WN         25  ...      6       10          0   \n",
      "\n",
      "   CancellationCode  Diverted  CarrierDelay WeatherDelay NASDelay  \\\n",
      "0               NaN         0          16.0          0.0      0.0   \n",
      "1               NaN         0           NaN          NaN      NaN   \n",
      "2               NaN         0           NaN          NaN      NaN   \n",
      "3               NaN         0           NaN          NaN      NaN   \n",
      "4               NaN         0          16.0          0.0      0.0   \n",
      "\n",
      "   SecurityDelay  LateAircraftDelay  \n",
      "0            0.0                0.0  \n",
      "1            NaN                NaN  \n",
      "2            NaN                NaN  \n",
      "3            NaN                NaN  \n",
      "4            0.0                0.0  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "   Year  Month  DayofMonth  DayOfWeek  DepTime  CRSDepTime  ArrTime  \\\n",
      "0  2008      1           3          4     1343        1325     1451   \n",
      "1  2008      1           3          4     1125        1120     1247   \n",
      "2  2008      1           3          4     2009        2015     2136   \n",
      "3  2008      1           3          4      903         855     1203   \n",
      "4  2008      1           3          4     1423        1400     1726   \n",
      "\n",
      "   CRSArrTime UniqueCarrier  FlightNum  ... TaxiIn  TaxiOut  Cancelled  \\\n",
      "0        1435            WN        588  ...      4        9          0   \n",
      "1        1245            WN       1343  ...      3        8          0   \n",
      "2        2140            WN       3841  ...      2       14          0   \n",
      "3        1205            WN          3  ...      5        7          0   \n",
      "4        1710            WN         25  ...      6       10          0   \n",
      "\n",
      "   CancellationCode  Diverted  CarrierDelay WeatherDelay NASDelay  \\\n",
      "0               NaN         0          16.0          0.0      0.0   \n",
      "1               NaN         0           NaN          NaN      NaN   \n",
      "2               NaN         0           NaN          NaN      NaN   \n",
      "3               NaN         0           NaN          NaN      NaN   \n",
      "4               NaN         0          16.0          0.0      0.0   \n",
      "\n",
      "   SecurityDelay  LateAircraftDelay  \n",
      "0            0.0                0.0  \n",
      "1            NaN                NaN  \n",
      "2            NaN                NaN  \n",
      "3            NaN                NaN  \n",
      "4            0.0                0.0  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sparkDF.limit(5).toPandas())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KRZYSZTOF - poprwić żeby działało na sparku :)\n",
    "# for column in ['DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime']:\n",
    "#     sparkDF[column] = sparkDF[column] // 100 + (sparkDF[column] % 100) / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "  Jak widać w powyższej tabeli, niektóre kolumny zawierają dane tekstowe - UniqueCarrier, TailNum, Origin, Dest i CancellationCode. \\\n",
    "Z racji tego, że w projekcie chcielibyśmy się skupić na powiązaniach między opóźnieniami/odwołaniami lotów, a momentem ich odbywania, część danych będzie nam zbędna. Dlatego też zdecydowaliśmy się na usunięcie kolumn:\n",
    "- UniqueCarrier - indywidualny kod przewoźnika\n",
    "- TailNum - numer ogonowy\n",
    "- Origin - miejsce rozpoczęcia podróży\n",
    "- Dest - cel podróży\n",
    "- CancellationCode - kod odwołania\n",
    "\n",
    "Ponadto usuwamy także poniższe kolumny:\n",
    "- FlightNum - ponieważ pełni on rolę numeru ID, więc nie będzie miało większego sensu uwzględnianie go w modelu.\n",
    "- TaxiIn, TaxiOut - ponieważ dane na temat taksówki naszym zdaniem nie mają wpływu na opóźnienie/odwołanie lotu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[Year: int, Month: int, DayofMonth: int, DayOfWeek: int, DepTime: int, CRSDepTime: int, ArrTime: int, CRSArrTime: int, ActualElapsedTime: int, CRSElapsedTime: int, AirTime: int, ArrDelay: int, DepDelay: int, Distance: int, Cancelled: int, Diverted: int, CarrierDelay: int, WeatherDelay: int, NASDelay: int, SecurityDelay: int, LateAircraftDelay: int]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "DataFrame[Year: int, Month: int, DayofMonth: int, DayOfWeek: int, DepTime: int, CRSDepTime: int, ArrTime: int, CRSArrTime: int, ActualElapsedTime: int, CRSElapsedTime: int, AirTime: int, ArrDelay: int, DepDelay: int, Distance: int, Cancelled: int, Diverted: int, CarrierDelay: int, WeatherDelay: int, NASDelay: int, SecurityDelay: int, LateAircraftDelay: int]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDF.drop('UniqueCarrier', 'TailNum', 'Origin', 'Dest','CancellationCode', 'FlightNum', 'TaxiIn', 'TaxiOut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "Column<'Cancelled'>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "Column<'Cancelled'>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDF.Cancelled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "cancelledDF = sparkDF.filter(sparkDF.Cancelled==1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "notCancelledDF = sparkDF.filter(sparkDF.Cancelled==0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 54354)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\klaud\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"C:\\Users\\klaud\\AppData\\Local\\Programs\\Python\\Python37\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] Istniejące połączenie zostało gwałtownie zamknięte przez zdalnego hosta\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\klaud\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"C:\\Users\\klaud\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\py4j\\clientserver.py\", line 540, in send_command\n",
      "    \"Error while sending or receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\klaud\\AppData\\Local\\Programs\\Python\\Python37\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"C:\\Users\\klaud\\AppData\\Local\\Programs\\Python\\Python37\\lib\\socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"C:\\Users\\klaud\\AppData\\Local\\Programs\\Python\\Python37\\lib\\socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"C:\\Users\\klaud\\AppData\\Local\\Programs\\Python\\Python37\\lib\\socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\Users\\klaud\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\pyspark\\accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\Users\\klaud\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\pyspark\\accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"C:\\Users\\klaud\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\pyspark\\accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"C:\\Users\\klaud\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\pyspark\\serializers.py\", line 593, in read_int\n",
      "    length = stream.read(4)\n",
      "  File \"C:\\Users\\klaud\\AppData\\Local\\Programs\\Python\\Python37\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] Istniejące połączenie zostało gwałtownie zamknięte przez zdalnego hosta\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] Nie można nawiązać połączenia, ponieważ komputer docelowy aktywnie go odmawia",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001B[0m in \u001B[0;36mcollect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    816\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 817\u001B[1;33m             \u001B[0msock_info\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcollectToPython\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    818\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mBatchedSerializer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mCPickleSerializer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1321\u001B[0m         return_value = get_return_value(\n\u001B[1;32m-> 1322\u001B[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[0;32m   1323\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\pyspark\\sql\\utils.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    189\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 190\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    191\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\py4j\\protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    335\u001B[0m                 \u001B[1;34m\"An error occurred while calling {0}{1}{2}\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 336\u001B[1;33m                 format(target_id, \".\", name))\n\u001B[0m\u001B[0;32m    337\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mPy4JError\u001B[0m: An error occurred while calling o683.collectToPython",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_3744\\967482269.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mpandasDF\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcancelledDF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfraction\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtoPandas\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py\u001B[0m in \u001B[0;36mtoPandas\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    203\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    204\u001B[0m         \u001B[1;31m# Below is toPandas without Arrow optimization.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 205\u001B[1;33m         \u001B[0mpdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_records\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    206\u001B[0m         \u001B[0mcolumn_counter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCounter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001B[0m in \u001B[0;36mcollect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    815\u001B[0m         \"\"\"\n\u001B[0;32m    816\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 817\u001B[1;33m             \u001B[0msock_info\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcollectToPython\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    818\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mBatchedSerializer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mCPickleSerializer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    819\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\pyspark\\traceback_utils.py\u001B[0m in \u001B[0;36m__exit__\u001B[1;34m(self, type, value, tb)\u001B[0m\n\u001B[0;32m     79\u001B[0m         \u001B[0mSCCallSiteSync\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_spark_stack_depth\u001B[0m \u001B[1;33m-=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     80\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_spark_stack_depth\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 81\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_context\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msetCallSite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1318\u001B[0m             \u001B[0mproto\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEND_COMMAND_PART\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1319\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1320\u001B[1;33m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1321\u001B[0m         return_value = get_return_value(\n\u001B[0;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001B[1;32m~\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36msend_command\u001B[1;34m(self, command, retry, binary)\u001B[0m\n\u001B[0;32m   1034\u001B[0m          \u001B[1;32mif\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mbinary\u001B[0m\u001B[0;31m`\u001B[0m \u001B[1;32mis\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1035\u001B[0m         \"\"\"\n\u001B[1;32m-> 1036\u001B[1;33m         \u001B[0mconnection\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_connection\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1037\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1038\u001B[0m             \u001B[0mresponse\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconnection\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\py4j\\clientserver.py\u001B[0m in \u001B[0;36m_get_connection\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    282\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    283\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mconnection\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mconnection\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msocket\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 284\u001B[1;33m             \u001B[0mconnection\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_create_new_connection\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    285\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mconnection\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    286\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\py4j\\clientserver.py\u001B[0m in \u001B[0;36m_create_new_connection\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    289\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjava_parameters\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython_parameters\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    290\u001B[0m             self.gateway_property, self)\n\u001B[1;32m--> 291\u001B[1;33m         \u001B[0mconnection\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconnect_to_java_server\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    292\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_thread_connection\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconnection\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    293\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mconnection\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\MOW\\BigDataMachineLearning\\venv\\lib\\site-packages\\py4j\\clientserver.py\u001B[0m in \u001B[0;36mconnect_to_java_server\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    436\u001B[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001B[0;32m    437\u001B[0m                     self.socket, server_hostname=self.java_address)\n\u001B[1;32m--> 438\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msocket\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjava_address\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjava_port\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    439\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstream\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msocket\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmakefile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"rb\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    440\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_connected\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mConnectionRefusedError\u001B[0m: [WinError 10061] Nie można nawiązać połączenia, ponieważ komputer docelowy aktywnie go odmawia"
     ]
    }
   ],
   "source": [
    "pandasDF = cancelledDF.sample(fraction=0.2).toPandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pandasDFNC = notCancelledDF.sample(fraction=0.2).toPandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:.2f}'.format, 'display.max_rows', None, 'display.max_columns', None):\n",
    "\n",
    "    display(pandasDF.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Jak widać w powyższej tabeli dane obejmują okres od stycznia do czerwca 2007 i są dość równo rozłożone w tym okresie - średni miesiąc to między marcem a kwietniem, dni miesiąca oraz dni tygodnia mają równo rozłożone kwartyle. \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## WIZUALIZACJA DANYCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### BOXPLOTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_visualisation = [column for column in pandasDF.columns if column != 'Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "for i, column in enumerate(columns_for_visualisation):\n",
    "    sns.boxplot(pandasDF[column], ax=axes[i], orient='v')\n",
    "    axes[i].set_title(column, fontsize=15)\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].set_xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### TO DO: Dodać komentarz dotyczący boxplotów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### HISTOGRAMY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "for i, column in enumerate(columns_for_visualisation):\n",
    "    sns.histplot(pandasDF[column], ax=axes[i], bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### TO DO: Dodać komentarz dotyczący histogramów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### SCATTER PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_scatter = ['DepTime', 'CRSDepTime',\n",
    "       'ArrTime', 'CRSArrTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#scatter plot matrix\n",
    "sns.pairplot(pandasDF, vars=columns_for_scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### TO DO: Dodać komentarz dotyczący scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## PROBLEMY Z DANYMI - DANE BRAKUJĄCE, NIEPRAWIDŁOWE, ODSTAJĄCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### SPRAWDZENIE POPRAWNOŚCI TYPÓW DANYCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(pandasDF)\n",
    "print(table.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Jak widać wyżej - wszystkie dane występują w poprawnym formacie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### NAPRAWA WIERSZY Z PUSTYMI DANYMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obliczenie ilosci pustych danych\n",
    "# Obliczenie ilosci pustych danych\n",
    "# # KRZYSZTOF - zrobic usuwanie pustych w sparku\n",
    "# np.sum(data.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sizeBeforeDeleteNull= data.count()\n",
    "# dataWithoutNull = data.dropna()\n",
    "# sizeAfterDeleteNull =  dataWithoutNull.count()\n",
    "# print(\"Usunieto: \", sizeBeforeDeleteNull - sizeAfterDeleteNull)\n",
    "#\n",
    "# print(\"Percent od reduced rows: \", 100*sum(sizeBeforeDeleteNull - sizeAfterDeleteNull)/sum(sizeBeforeDeleteNull))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### TO DO: ZMIENIC SPOSÓB RADZENIA SOBIE Z PUSTYMI DANYMI\n",
    "### TO DO: Dodać komentarz dotyczący danych pustych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### USUWANIE DANYCH ODSTAJĄCYCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sizeBefore = np.shape(data)[0]\n",
    "# for col in data.columns:\n",
    "#     data = data[np.abs(data[col]-data[col].mean()) <= (3*data[col].std())]\n",
    "# sizeAfter =  np.shape(data)[0]\n",
    "# print(\"Count of reduced rows: \", sizeBefore - sizeAfter)\n",
    "# print(\"Percent od reduced rows: \", 100*(sizeBefore - sizeAfter)/sizeBefore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### TO DO: Dodać komentarz dotyczący danych odstających"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# KORELACJE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20), dpi=80)\n",
    "corrMatrix = sparkDF.corr()\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### TO DO: Dodać komentarz dotyczący korelacji między danymi, ew dodać pairploty do wybranych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO: \"Normalizacja danych (przedstawić wyniki min-max i standaryzacji). Zastanowić się nad zakresem skalowania danych\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## REDUKCJA WYMIAROWOŚCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### TO DO: Genetyczna optymalizacja cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Genetyczna optymalizacja cech - NA RAZIE SAMA SELEKCJA NA PODSTAWIE KORELACJI\n",
    "sparkDF = sparkDF.filter([\"DayOfWeek\", \"DayofMonth\", \"Distance\", \"DepTime\", \"Cancelled\", \"Diverted\", \"LateAircraftDelay\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# PRZYGOTOWANIE MODELI REGRESJI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### PODZIAŁ DANYCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "vector = VectorAssembler(inputCols = ['DayOfWeek', 'DayofMonth', 'Distance', 'DepTime', 'Cancelled', 'Diverted'\n",
    "], outputCol = 'features')\n",
    "vectorData = vector.transform(sparkDF).select(['features', 'LateAircraftDelay'])\n",
    "vectorData.plot.bar(x = 'parameter', y= 'Coefficients')\n",
    "plt.show()\n",
    "\n",
    "vectorData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df, test_df = sparkDF.randomSplit([0.7, 0.3])\n",
    "print(\"train.rows: \", train_df.count())\n",
    "print(\"test.rows: \", test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### MIARY SKUTECZNOŚCI\n",
    "\n",
    "evaluatorRMSE = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluatorR2 = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "\n",
    "def effectivenessMeasures(model, predictions):\n",
    "    print(\"Coefficients: \" + str(model.coefficients))\n",
    "    print(\"Intercept: \" + str(model.intercept))\n",
    "    print(predictions.show(5))\n",
    "    rmse = evaluatorRMSE.evaluate(predictions)\n",
    "    r2 = evaluatorR2.evaluate(predictions)\n",
    "    print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "    print(\"R2 on test data = %g\" % r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "linear_reg = LinearRegression(featuresCol = 'features', labelCol='label', maxIter=20, regParam=0.3, elasticNetParam=0.8)\n",
    "linear_model = linear_reg.fit(train_df)\n",
    "linear_predictions = linear_model.transform(test_df)\n",
    "effectivenessMeasures(linear_model, linear_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## RANDOM FOREST regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol = 'features', labelCol='label')\n",
    "\n",
    "rf_model = rf.fit(train_df)\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "\n",
    "effectivenessMeasures(rf_model, rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## DECISION TREE REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(featuresCol = 'features', labelCol='label')\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "\n",
    "effectivenessMeasures(dt_model, dt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KLASYFIKACJA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MIARY SKUTECZNOŚCI - KLASYFIKACJA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def effectivenessMeasuresClassifier(predictions):\n",
    "    predictionCol = \"features\"\n",
    "    labelCol = \"Cancelled\"\n",
    "    acc=MulticlassClassificationEvaluator(predictionCol=predictionCol, labelCol=labelCol, metricName='accuracy').evaluate(predictions)\n",
    "    print(\"Prediction Accuracy: \", acc)\n",
    "\n",
    "    f1=MulticlassClassificationEvaluator(predictionCol=predictionCol, labelCol=labelCol, metricName='f1').evaluate(predictions)\n",
    "    print(\"F1: \", f1)\n",
    "\n",
    "    precision=MulticlassClassificationEvaluator(predictionCol=predictionCol, labelCol=labelCol, metricName='weightedPrecision').evaluate(predictions)\n",
    "    print(\"Precision: \", precision)\n",
    "\n",
    "    recall=MulticlassClassificationEvaluator(predictionCol=predictionCol, labelCol=labelCol, metricName='weightedRecall').evaluate(predictions)\n",
    "    print(\"Recall: \", recall)\n",
    "\n",
    "    y_pred=predictions.select(predictionCol).collect()\n",
    "    y_orig=predictions.select(labelCol).collect()\n",
    "\n",
    "    cm = confusion_matrix(y_orig, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    return acc, f1, precision, recall\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DECISION TREE CLASSIFIER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "(trainingData, testData) = vectorData.randomSplit([0.7, 0.3])\n",
    "dtc = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Cancelled\").fit(trainingData)\n",
    "pred = dtc.transform(testData)\n",
    "# pred.show()\n",
    "eff_dtc = effectivenessMeasuresClassifier(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RANDOM FOREST CLASSIFIER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(labelCol=\"Cancelled\", featuresCol=\"features\", numTrees=10).fit(trainingData)\n",
    "pred = rfc.transform(testData)\n",
    "eff_rfc = effectivenessMeasuresClassifier(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MULTILAYER PERCEPTRON CLASSIFIER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mpc = MultilayerPerceptronClassifier(labelCol=\"Cancelled\", featuresCol=\"features\", numTrees=10).fit(trainingData)\n",
    "pred = mpc.transform(testData)\n",
    "eff_mpc = effectivenessMeasuresClassifier(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GBT CLASSIFIER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=\"Cancelled\", featuresCol=\"features\", numTrees=10).fit(trainingData)\n",
    "pred = gbt.transform(testData)\n",
    "eff_gbt = effectivenessMeasuresClassifier(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ENSAMBLE CLASSIFICATOR - EXTRA TREES CLASSIFIER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etr = ExtraTreesClassifier(n_estimators = 100, criterion ='mse', max_features = \"auto\")\n",
    "etr_model = etr.fit(trainingData)\n",
    "pred = etr_model.transform(testData)\n",
    "eff_etr = effectivenessMeasuresClassifier(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ENSAMBLE CLASSIFICATOR - VOTING CLASSIFIER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "lr_model = linear_model.LinearClassifier()\n",
    "dc_model = DecisionTreeClassifier()\n",
    "rf_model = RandomForestClassifier()\n",
    "estimators = [('lr', lr_model), ('dc', dc_model), ('rf', rf_model)]\n",
    "vc = VotingClassifier(estimators)\n",
    "vc_model = vc.fit(trainingData)\n",
    "pred = vc_model.transform(testData)\n",
    "vc_etr = effectivenessMeasuresClassifier(pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ENSAMBLE CLASSIFICATOR - STACKING CLASSIFIER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "lr_model = linear_model.LinearClassifier()\n",
    "dc_model = DecisionTreeClassifier()\n",
    "rf_model = RandomForestClassifier()\n",
    "estimators = [('lr', lr_model), ('dc', dc_model), ('rf', rf_model)]\n",
    "sc = StackingClassifier(estimators)\n",
    "sc_model = sc.fit(trainingData)\n",
    "pred = sc_model.transform(testData)\n",
    "sc_etr = effectivenessMeasuresClassifier(pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-KROTNA WALIDAJA KRZYŻOWA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "\n",
    "def cross_validation_split(data, folds):\n",
    "    dataset = data.copy().to_numpy()\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / folds)\n",
    "    for i in range(folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TO DO STWORZYC FUNKCJE GENERUJACA WYNIKI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: optymalizacja parametrow klasyfikatorow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.bar(trainingData.columns, etr.feature_importance)\n",
    "plt.xticks(rotation=40)\n",
    "plt.xlabel('Feature Labels')\n",
    "plt.ylabel('Feature Importances')\n",
    "plt.title('Comparison of different Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "corrMatrix = sparkDF.corr()\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.xlabel('Feature Labels')\n",
    "plt.ylabel('Feature Importances')\n",
    "plt.title('Comparison of different Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(sparkDF.columns[:11], corrMatrix[\"Cancelled\"][:11])\n",
    "plt.xticks(rotation=40)\n",
    "plt.xlabel('Feature Labels')\n",
    "plt.ylabel('Feature Importances')\n",
    "plt.title('Comparison of different Feature Importances')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "size_before_opt = np.shape(sparkDF)[1] - 1\n",
    "print(\"Ilosc wymairów zmiennych zależnych przed optymalizacją: \", size_before_opt)\n",
    "data.drop('season', inplace=True, axis=1)\n",
    "data.drop('mnth', inplace=True, axis=1)\n",
    "data.drop('holiday', inplace=True, axis=1)\n",
    "data.drop('weathersit', inplace=True, axis=1)\n",
    "data.drop('temp', inplace=True, axis=1)\n",
    "data.drop('hum', inplace=True, axis=1)\n",
    "data.drop('windspeed', inplace=True, axis=1)\n",
    "size_after_opt = np.shape(data)[1] - 1\n",
    "print(\"Ilosc wymairów zmiennych zależnych po optymalizacji: \",size_after_opt)\n",
    "\n",
    "x_train, x_test, y_train, y_test = splitOfData(data, 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def add_row(table, result, model):\n",
    "    avg_result = [sum(x) / len(x) for x in zip(*result)]\n",
    "    table.add_row([model, round(avg_result[0],5), round(avg_result[1],5), round(avg_result[2],5)])\n",
    "\n",
    "def createSummaryTable(summary_table, results):\n",
    "    add_row(summary_table, results[0], \"Linear Regression\")\n",
    "    add_row(summary_table, results[1], \"Polynominal Regression\")\n",
    "    add_row(summary_table, results[2], \"Decision Tree Regression\")\n",
    "    add_row(summary_table, results[3], \"Random Forrest Regression\")\n",
    "    add_row(summary_table, results[4], \"Voting Regressor\")\n",
    "    add_row(summary_table, results[5], \"Stacking Regressor\")\n",
    "\n",
    "def createSummary(k_fold = 0, grid_search_optimalization = False):\n",
    "    summary_table = PrettyTable(['model', 'MSE', 'r2', 'Experience Variance'])\n",
    "    if (k_fold < 2):\n",
    "        results = prepare_result(grid_search_optimalization)\n",
    "        createSummaryTable(summary_table, results)\n",
    "        print(\"Summary table for result of regression models:\\n\", summary_table, \"\\n\\n\")\n",
    "    else:\n",
    "        results = prepare_result_with_k_fold(k_fold, grid_search_optimalization)\n",
    "        createSummaryTable(summary_table, results)\n",
    "        print(\"Summary table for result of regression models [K_fold: k =\",k_fold,\"]:\\n\", summary_table, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "createSummary()\n",
    "createSummary(k_fold = 2)\n",
    "createSummary(k_fold = 5)\n",
    "createSummary(k_fold = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8be385d48e05b4f2d5b0c548e070dbf7e445fa615772f44dee8b1f60efa8125"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
